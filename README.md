Abstract
	In the world of natural language processing, much effort has been put in place to understand how customers react to products, and where their interests lay. This paper focuses on a specific use case of natural language processes where we constructed a model using data from customer reviews on Amazon to predict, using text alone, if we are able to predict if a customer liked a certain product or not. By using simple to moderately complex features on a relatively small dataset, we were able to create good results and predict customer sentiment with high confidence. The following is an explanation of the data set, methodology, and results.
 
Introduction
  Customer sentiment is often the most important metric that many businesses look at to evaluate how well their company or products are doing. Many large corporations send our surveys to their customers with a series of questions asking how they feel about the company. Every restaurant and online retailer cares immensely about reviews left on their store or product since those with the highest reviews (in theory) are likely to get the most attention, foot traffic, and products sold. For these reasons, we decided to see if we can build an algorithm using some natural language processing techniques to see if we can evaluate whether a customer’s sentiment is positive or negative based on their words alone. We believe this is possible to do since as humans, we can very easily understand if what we are readying is a positive review of a product or a negative one. This implies that there are key differences between these two types of reviews that can be fed into a model to help make these decisions in aggregate.
We have implemented our Sentiment Analysis on Amazon food review using logistic regression,decision trees, random forest machine learning algorithms.

The following features have been considered for implementing:
1.Bag of Positive and Negative words: A set of positive and negative words have been taken and for each review if the token matches in the set of positive and negative words it is counted and stored separately as the number of positive and negative counts in the review.The set of positive and negative words taken online which has 2005 and 4783 words respectively. In a review if it is positive then chance of writing good words is high, so there is highly likely the words can be found from positive word bag and  review if it is negative then chance of writing negative words is high, so there is highly likely the words can be found from negative words bag 
2. Word Count:  Count of all words in the review is taken by splitting the review and removing the punctuation marks. Each review is different compared to other reviews. So the length of the review changes with different reviews. So the count of the review is taken into consideration.
3.Stopword Count: Counted the number of stopwords in each review by checking the words in the list of stop words(English) which are imported from NLTK.  When finding the relation between the reviews, if the words are matched between the two reviews the more number of matching words in the two reviews are stop words, the accuracy will be more. Hence the stopwords are removed.
4.Sentence Count: The Number of sentences i.e,count in each review are taken.
5.Unique Words Count: Each review has repeated words, counting the words will have repeatedness, the count of every unique word is taken.
6.As Adjectives can give the meaning of the supporting word as either positive or negative, the count of adjectives is considered. The Count of adjectives in the first half of the review and second half and count of total adjectives in the review is also taken. 

Conclusion
	The goal of our experimentation was to see if we are able to construct a machine learning algorithm that can predict customer sentiment based on their written reviews. We began with a corpus of real Amazon customer reviews with their associated sentiments. We split this corpus into an 80-20 split and created six features to accommodate our model and help boost results. We experimented with a total of three models: Decision Tree, Logistic Regression, and RandomForest. All of our model iterations performed fairly well with our worst notable iteration having an overall accuracy of 75%. All our models had a very high degree of precision, with a bit lower recall. It is worth noting that although there is ~10% difference between the two metrics, we were not concerned about our models’ reliability due both values being in the 90s and 80s respectively. In the end we settled on our RandomForest model fine tuned with Count and embeddings to be our preferred model since it had the highest accuracy and balance of precision and recall compared to the other model iterations performed.
 
Dataset Source:
https://www.kaggle.com/code/laowingkin/amazon-fine-food-review-sentiment-analysis/data
